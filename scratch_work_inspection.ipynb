{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from taming.models.vqgan import VQModel\n",
    "import argparse, os, sys, datetime, glob, importlib\n",
    "from omegaconf import OmegaConf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import seed_everything\n",
    "from pytorch_lightning.trainer import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, Callback, LearningRateMonitor\n",
    "from pytorch_lightning.utilities import rank_zero_only\n",
    "\n",
    "from taming.data.utils import custom_collate\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_config_path = [\"configs/custom_vqgan_eval.yaml\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obj_from_str(string, reload=False):\n",
    "    module, cls = string.rsplit(\".\", 1)\n",
    "    if reload:\n",
    "        module_imp = importlib.import_module(module)\n",
    "        importlib.reload(module_imp)\n",
    "    return getattr(importlib.import_module(module, package=None), cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instantiate_from_config(config):\n",
    "    if not \"target\" in config:\n",
    "        raise KeyError(\"Expected key `target` to instantiate.\")\n",
    "    return get_obj_from_str(config[\"target\"])(**config.get(\"params\", dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_local(self, save_dir, split, images, global_step, current_epoch, batch_idx):\n",
    "    root = os.path.join(save_dir, \"images\", split)\n",
    "    for k in images:\n",
    "        grid = torchvision.utils.make_grid(images[k], nrow=4)\n",
    "\n",
    "        grid = (grid+1.0)/2.0 # -1,1 -> 0,1; c,h,w\n",
    "        grid = grid.transpose(0,1).transpose(1,2).squeeze(-1)\n",
    "        grid = grid.numpy()\n",
    "        grid = (grid*255).astype(np.uint8)\n",
    "        filename = \"{}_gs-{:06}_e-{:06}_b-{:06}.png\".format(\n",
    "            k,\n",
    "            global_step,\n",
    "            current_epoch,\n",
    "            batch_idx)\n",
    "        path = os.path.join(root, filename)\n",
    "        os.makedirs(os.path.split(path)[0], exist_ok=True)\n",
    "        Image.fromarray(grid).save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [OmegaConf.load(cfg) for cfg in base_config_path]\n",
    "config = OmegaConf.merge(*configs)\n",
    "\n",
    "# config.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = instantiate_from_config(config.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data of interest\n",
    "data = instantiate_from_config(config.data)\n",
    "# NOTE according to https://pytorch-lightning.readthedocs.io/en/latest/datamodules.html\n",
    "# calling these ourselves should not be necessary but it is.\n",
    "# lightning still takes care of proper multiprocessing though\n",
    "data.prepare_data()\n",
    "data.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_loader = data.val_dataloader()\n",
    "train_data_loader = data.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data_loader.__len__())\n",
    "print(train_data_loader.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the models to eval mode\n",
    "model.eval()\n",
    "model.loss.discriminator.eval()\n",
    "\n",
    "# make a directory to hold qualtative eval images\n",
    "output_path = \"/work/pi_hzhang2_umass_edu/oyoungquist_umass_edu/taming-transformers/vqgan_eval/DeepAccident/negatives/\"\n",
    "\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "batch_num = 0\n",
    "\n",
    "# iterate over the test set\n",
    "for batch in test_data_loader:\n",
    "    print(batch_num)\n",
    "    # don't track the gradient\n",
    "    with torch.no_grad():\n",
    "        log_dict = model.log_images(batch)\n",
    "    # plt.imshow(batch[\"image\"][2].numpy())\n",
    "\n",
    "    # print(log_dict[\"dis_diff\"])\n",
    "    # print(log_dict[\"feature_diff\"].size())\n",
    "    \n",
    "    log_dict[\"inputs\"] = log_dict[\"inputs\"].permute(0,2,3,1)\n",
    "    log_dict[\"reconstructions\"] = log_dict[\"reconstructions\"].permute(0,2,3,1)\n",
    "    log_dict[\"feature_diff\"] = log_dict[\"feature_diff\"].permute(0,2,3,1)\n",
    "    \n",
    "    \n",
    "    # print(log_dict[\"feature_diff\"].size())\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, squeeze=True)\n",
    "    \n",
    "    axes[0].imshow(log_dict[\"inputs\"][0].numpy())\n",
    "    axes[0].axis('off')\n",
    "    axes[0].set_title(\"Input\")\n",
    "    axes[1].imshow(log_dict[\"reconstructions\"][0].numpy())\n",
    "    axes[1].axis('off')\n",
    "    axes[1].set_title(\"Recon.\")\n",
    "    axes[2].imshow(log_dict[\"feature_diff\"][0].numpy())\n",
    "    axes[2].axis('off')\n",
    "    axes[2].set_title(\"Diff.\")\n",
    "    new_path = os.path.join(output_path, \"neg_eval_sample_{:d}.png\".format(batch_num))\n",
    "    batch_num += 1\n",
    "    fig.savefig(new_path)\n",
    "    plt.close()\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, squeeze=True)\n",
    "    \n",
    "    axes[0].imshow(log_dict[\"inputs\"][1].numpy())\n",
    "    axes[0].axis('off')\n",
    "    axes[0].set_title(\"Input\")\n",
    "    axes[1].imshow(log_dict[\"reconstructions\"][1].numpy())\n",
    "    axes[1].axis('off')\n",
    "    axes[1].set_title(\"Recon.\")\n",
    "    axes[2].imshow(log_dict[\"feature_diff\"][1].numpy())\n",
    "    axes[2].axis('off')\n",
    "    axes[2].set_title(\"Diff.\")\n",
    "    new_path = os.path.join(output_path, \"neg_eval_sample_{:d}.png\".format(batch_num))\n",
    "    batch_num += 1\n",
    "    fig.savefig(new_path)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, squeeze=True)\n",
    "    \n",
    "    axes[0].imshow(log_dict[\"inputs\"][2].numpy())\n",
    "    axes[0].axis('off')\n",
    "    axes[0].set_title(\"Input\")\n",
    "    axes[1].imshow(log_dict[\"reconstructions\"][2].numpy())\n",
    "    axes[1].axis('off')\n",
    "    axes[1].set_title(\"Recon.\")\n",
    "    axes[2].imshow(log_dict[\"feature_diff\"][2].numpy())\n",
    "    axes[2].axis('off')\n",
    "    axes[2].set_title(\"Diff.\")\n",
    "    new_path = os.path.join(output_path, \"neg_eval_sample_{:d}.png\".format(batch_num))\n",
    "    batch_num += 1\n",
    "    fig.savefig(new_path)\n",
    "    plt.close()\n",
    "    # plt.show()\n",
    "\n",
    "    if (batch_num > 5):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taming",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
